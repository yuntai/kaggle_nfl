{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pathlib     \nimport pandas as pd\nimport numpy as np \nimport tqdm        \nimport torch       \nimport gzip\nimport os                                             \nimport json                                           \nimport time                                           \nimport gzip                                           \nimport pprint                                         \nimport argparse                                       \n                                                      \nimport numpy as np                                    \nimport torch                                          \nfrom torch.utils.data import TensorDataset, DataLoader, Dataset\nfrom torch import optim                               \nimport torch.nn as nn                                 \nimport torch.nn.functional as F                       \nimport sklearn                                        \nfrom sklearn.utils import resample\n\nimport torch                          \nfrom torch.nn.utils import weight_norm\n\n\ndef CRPSLoss(y_pred, y):                                                        \n    return torch.sum((y_pred.cumsum(-1) - y.cumsum(-1))**2) / (199 * y_pred.shape[0])\n                                                                                \nYARDS_CLIP = [-15, 50]\n\ndevice = 'cpu'\n\ngrad_clip = 0.25\nlr_max = 0.001\nlr = 0.001\neta_min = 1e-7\n\nmax_epochs = 40\nn_bagging = 4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# FROM https://github.com/dkumazaw/onecyclelr\n# License: https://github.com/dkumazaw/onecyclelr/blob/master/LICENSE\nfrom torch.optim import Optimizer\n\n\nclass OneCycleLR:\n    \"\"\" Sets the learing rate of each parameter group by the one cycle learning rate policy\n    proposed in https://arxiv.org/pdf/1708.07120.pdf. \n\n    It is recommended that you set the max_lr to be the learning rate that achieves \n    the lowest loss in the learning rate range test, and set min_lr to be 1/10 th of max_lr.\n\n    So, the learning rate changes like min_lr -> max_lr -> min_lr -> final_lr, \n    where final_lr = min_lr * reduce_factor.\n\n    Note: Currently only supports one parameter group.\n\n    Args:\n        optimizer:             (Optimizer) against which we apply this scheduler\n        num_steps:             (int) of total number of steps/iterations\n        lr_range:              (tuple) of min and max values of learning rate\n        momentum_range:        (tuple) of min and max values of momentum\n        annihilation_frac:     (float), fracion of steps to annihilate the learning rate\n        reduce_factor:         (float), denotes the factor by which we annihilate the learning rate at the end\n        last_step:             (int), denotes the last step. Set to -1 to start training from the beginning\n\n    Example:\n        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n        >>> scheduler = OneCycleLR(optimizer, num_steps=num_steps, lr_range=(0.1, 1.))\n        >>> for epoch in range(epochs):\n        >>>     for step in train_dataloader:\n        >>>         train(...)\n        >>>         scheduler.step()\n\n    Useful resources:\n        https://towardsdatascience.com/finding-good-learning-rate-and-the-one-cycle-policy-7159fe1db5d6\n        https://medium.com/vitalify-asia/whats-up-with-deep-learning-optimizers-since-adam-5c1d862b9db0\n    \"\"\"\n\n    def __init__(self,\n                 optimizer: Optimizer,\n                 num_steps: int,\n                 lr_range: tuple = (0.1, 1.),\n                 momentum_range: tuple = (0.85, 0.95),\n                 annihilation_frac: float = 0.1,\n                 reduce_factor: float = 0.01,\n                 last_step: int = -1):\n        # Sanity check\n        if not isinstance(optimizer, Optimizer):\n            raise TypeError('{} is not an Optimizer'.format(type(optimizer).__name__))\n        self.optimizer = optimizer\n\n        self.num_steps = num_steps\n\n        self.min_lr, self.max_lr = lr_range[0], lr_range[1]\n        assert self.min_lr < self.max_lr, \\\n            \"Argument lr_range must be (min_lr, max_lr), where min_lr < max_lr\"\n\n        self.min_momentum, self.max_momentum = momentum_range[0], momentum_range[1]\n        assert self.min_momentum < self.max_momentum, \\\n            \"Argument momentum_range must be (min_momentum, max_momentum), where min_momentum < max_momentum\"\n\n        self.num_cycle_steps = int(num_steps * (1. - annihilation_frac))  # Total number of steps in the cycle\n        self.final_lr = self.min_lr * reduce_factor\n\n        self.last_step = last_step\n\n        if self.last_step == -1:\n            self.step()\n\n    def state_dict(self):\n        \"\"\"Returns the state of the scheduler as a :class:`dict`.\n        It contains an entry for every variable in self.__dict__ which\n        is not the optimizer. (Borrowed from _LRScheduler class in torch.optim.lr_scheduler.py)\n        \"\"\"\n        return {key: value for key, value in self.__dict__.items() if key != 'optimizer'}\n\n    def load_state_dict(self, state_dict):\n        \"\"\"Loads the schedulers state. (Borrowed from _LRScheduler class in torch.optim.lr_scheduler.py)\n        Arguments:\n            state_dict (dict): scheduler state. Should be an object returned\n                from a call to :meth:`state_dict`.\n        \"\"\"\n        self.__dict__.update(state_dict)\n\n    def get_lr(self):\n        return self.optimizer.param_groups[0]['lr']\n\n    def get_momentum(self):\n        return self.optimizer.param_groups[0]['momentum']\n\n    def step(self):\n        \"\"\"Conducts one step of learning rate and momentum update\n        \"\"\"\n        current_step = self.last_step + 1\n        self.last_step = current_step\n\n        if current_step <= self.num_cycle_steps // 2:\n            # Scale up phase\n            scale = current_step / (self.num_cycle_steps // 2)\n            lr = self.min_lr + (self.max_lr - self.min_lr) * scale\n            momentum = self.max_momentum - (self.max_momentum - self.min_momentum) * scale\n        elif current_step <= self.num_cycle_steps:\n            # Scale down phase\n            scale = (current_step - self.num_cycle_steps // 2) / (self.num_cycle_steps - self.num_cycle_steps // 2)\n            lr = self.max_lr - (self.max_lr - self.min_lr) * scale\n            momentum = self.min_momentum + (self.max_momentum - self.min_momentum) * scale\n        elif current_step <= self.num_steps:\n            # Annihilation phase: only change lr\n            scale = (current_step - self.num_cycle_steps) / (self.num_steps - self.num_cycle_steps)\n            lr = self.min_lr - (self.min_lr - self.final_lr) * scale\n            momentum = None\n        else:\n            # Exceeded given num_steps: do nothing\n            return\n\n        self.optimizer.param_groups[0]['lr'] = lr\n        if momentum:\n            self.optimizer.param_groups[0]['momentum'] = momentum","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def preprocess(df, predicting=False):                                                                                                                                      \n    # fix team abbr                                                             \n    abbr_corrections = {'BLT': 'BAL', 'CLV': 'CLE', 'ARZ': 'ARI', 'HST': 'HOU'} \n    for k, v in abbr_corrections.items():                                       \n        df.loc[df.PossessionTeam == k, 'PossessionTeam'] = v                    \n        df.loc[df.FieldPosition == k, 'FieldPosition'] = v                      \n        df.loc[df.HomeTeamAbbr == k, 'HomeTeamAbbr'] = v                        \n                                                                                \n    # find offender/defender and rusher                                         \n    df['Offender'] = (df.HomeTeamAbbr == df.PossessionTeam) & (df.Team == 'home') | (df.HomeTeamAbbr != df.PossessionTeam) & (df.Team == 'away')\n    df['Rusher'] = df.NflIdRusher == df.NflId                                   \n                                                                                \n    # some NA in Dir; should be okay as speed = 0                               \n    assert df[df.Dir.isna() & (df.S != 0.0)].size == 0                          \n    df.Dir.fillna(0., inplace=True)                                             \n                                                                                \n    df['dir'] = -(df.Dir * np.pi/180. - np.pi/2.) # adjusted & radian           \n    df['Y_aug'] = 53.33 - df['Y'] # y coordinates flipe                         \n                                                                                \n    # field postion NA when YardLine = 50                                       \n    assert df.loc[df.FieldPosition.isna() & (df.YardLine != 50)].shape[0] == 0  \n                                                                                \n    # adjust yardline to x-axis                                                 \n    __mask = ~df.FieldPosition.isna() & ((df.FieldPosition != df.PossessionTeam) & (df.PlayDirection == 'right') | (df.FieldPosition == df.PossessionTeam) & (df.PlayDirection == 'left'))\n    df.loc[__mask, 'YardLine'] = 100 - df.loc[__mask, 'YardLine']               \n                                                                                \n    # fix Speed column for 2017 season                                          \n    __2017_season = df.Season == 2017                                           \n    df.loc[__2017_season, 'S'] = 10 * df.loc[__2017_season, 'Dis']              \n                                                                                \n    df['dx'] = df.S * np.cos(df.dir)                                            \n    df['dy'] = df.S * np.sin(df.dir)                                            \n    df['dy_aug'] = -df['dy']                                                    \n                                                                                \n    # make it always from left to right                                         \n    __mask = (df.PlayDirection == 'left')                                       \n    df.loc[__mask, 'X'] = 120 - df.loc[__mask, 'X'] # range 0 ~ 120             \n    df.loc[__mask, 'Y'] = 53.33 - df.loc[__mask, 'Y']                           \n    df.loc[__mask, 'Y_aug'] = 53.33 - df.loc[__mask, 'Y_aug']                   \n    df.loc[__mask, 'dx'] = -df.loc[__mask, 'dx']                                \n    df.loc[__mask, 'dy'] = -df.loc[__mask, 'dy']                                \n    df.loc[__mask, 'dy_aug'] = -df.loc[__mask, 'dy_aug']                        \n    df.loc[__mask, 'YardLine'] = 100 - df.loc[__mask, 'YardLine']               \n                                                                                \n    # create augmented feature for all rows and select during dfing             \n    play_group = df.groupby('PlayId', sort=True)\n    \n    play_df_cols = (['Yards'] if not predicting else []) + ['YardLine', 'Season', 'GameId']\n    play_df = play_group[play_df_cols].first().reset_index()\n    if not predicting:\n        play_df['YardsClipped'] = play_df.Yards.clip(YARDS_CLIP[0], YARDS_CLIP[1])  \n                                                                                \n    cols = ['X', 'Y', 'dx', 'dy', 'X', 'Y_aug', 'dx', 'dy_aug']                 \n                                                                                \n    features = []                                                               \n    features_aug = []                                                           \n    for _id, g in tqdm.tqdm(play_group):                                        \n        diffense = g.loc[~g.Offender, cols].values                              \n        offense = g.loc[g.Offender & ~g.Rusher, cols].values                    \n        rusher = g.loc[g.Rusher, cols].values                                   \n                                                                                \n        f12 = diffense[:,None] - offense[None]                                  \n        f34 = np.repeat((diffense - rusher)[:,None], repeats=10, axis=1)        \n        f5 = np.repeat(diffense[:,[2,3,6,7]][:,None], repeats=10, axis=1)       \n                                                                                \n        f     = np.concatenate([f12[...,:4], f34[...,:4], f5[...,:2]], axis=-1) \n        f_aug = np.concatenate([f12[...,4:], f34[...,4:], f5[...,2:]], axis=-1) \n                                                                                \n        features.append(f)                                                      \n        features_aug.append(f_aug)                                              \n                                                                                \n    YARD_GRID = np.arange(-99, 100)[None]                                       \n    EYE = np.eye(YARDS_CLIP[1]-YARDS_CLIP[0]+1)                                 \n                                                                                \n    features = np.stack(features).transpose((0, 3, 1, 2)) # channel first       \n    features_aug = np.stack(features_aug).transpose((0, 3, 1, 2)) # channel first\n                                                                                \n    assert np.isnan(features).sum() == 0, f\"nan found in features\"              \n    assert np.isnan(features_aug).sum() == 0, f\"nan found in features\"          \n                                                                                \n    play_ids = play_df.PlayId.values\n    if not predicting:\n        yards = EYE[play_df.YardsClipped.values + -YARDS_CLIP[0]]\n        idxs_2017 = play_df.loc[play_df.Season == 2017].index.tolist()\n    yard_lines = play_df.YardLine.values\n    yard_mask = ((YARD_GRID <= (100 - yard_lines[:,None])) & (YARD_GRID >= -yard_lines[:,None])).astype(np.int)\n    game_ids = play_df.GameId.values                                            \n                  \n    \n    if not predicting:\n        D = [features, features_aug, yards, yard_mask, game_ids]\n    else:\n        D = [features, features_aug, yard_mask, game_ids]\n    assert all(D[0].shape[0] == d.shape[0] for d in D)\n    if not predicting:\n        D += [idxs_2017]                                                            \n                                                                                \n    return D\n\n\n\ndef run_epoch(model, loader, opt=None, scheduler=None, _epoch=-1):              \n    model.eval() if opt is None else model.train()                              \n    dev = next(model.parameters()).device                                       \n    batch_id = 0                                                                \n    total_loss = 0 # loss for log interval                                      \n    epoch_loss = 0                                                              \n                                                                                \n    with torch.enable_grad() if opt else torch.no_grad():                       \n        for _batch in loader:                                                   \n            X, y, mask = [t.to(dev) for t in _batch]                            \n            y_pred = model(X)                                                   \n            y_pred = F.softmax(y_pred, dim=-1)                                  \n            loss = CRPSLoss(y_pred, y)                                          \n            batch_loss = loss.detach().to('cpu').numpy()                        \n            epoch_loss += batch_loss                                            \n            total_loss += batch_loss                                            \n                                                                                \n            if opt:                                                             \n                opt.zero_grad()                                                 \n                loss.backward()                                                 \n                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n                opt.step()\n                scheduler.step()\n    return epoch_loss / len(loader)                                             \n                                                                                \ndef setup_train(max_steps):                                                     \n    model = NFLRushNet()                                                        \n    n_all_param = sum([p.nelement() for p in model.parameters() if p.requires_grad])\n    print(f'#params = {n_all_param/1e6:.2f}M')                                \n                                                                                \n    # initilize optimizer and learning rate scheduler                           \n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    #scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, max_steps, eta_min=eta_min)\n    scheduler = OneCycleLR(optimizer, lr_range=(0.0005, 0.001), num_steps=max_steps)\n                                                                                \n    model = model.to(device)                                           \n                                                                                \n    return model, optimizer, scheduler                                     \n                                                                                \ndef run_final(X, X_aug, y, mask, n_bagging=4, bagging_p=0.9, aug=True, aug_p=0.5, batch_size=64):\n    ixs_tr = list(range(X.shape[0]))                                            \n    n_samples = int(len(ixs_tr) * bagging_p)                                    \n               \n    models = []\n    random_state = 1000                                                         \n    for i in range(n_bagging):                                                  \n        ixs = resample(ixs_tr, n_samples=n_samples, random_state=random_state)  \n        train_dataset = RushDataset(X[ixs], X_aug[ixs], y[ixs], mask[ixs], aug=aug, aug_p=0.5)\n        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n        model, loss = train_loop(train_loader, bagging_ix=i)                           \n        print(\"train loss=\", loss)                                              \n        random_state += 1\n        models.append(model)\n    return models    \n        \n\ndef train_loop(train_loader, fold_ix=0, bagging_ix=0, meta={}):\n                                                                                \n    max_steps = max_epochs * len(train_loader)                             \n                                                                                \n    print(f'epoch step size = {len(train_loader)}')                           \n    print(f'max steps = {max_steps}')                                         \n                                                                                \n    model, opt, scheduler = setup_train(max_steps)                              \n                                                                                                                                                                                                                   \n    for epoch_i in range(max_epochs):                       \n        start = time.time()                                                     \n        train_loss = run_epoch(model, train_loader, opt, scheduler, _epoch=epoch_i)\n        end = time.time()                                                   \n        print(f\"{bagging_ix}/{epoch_i:2d}|{end-start:.2f}s|lr({opt.param_groups[0]['lr']:.7f})|loss({train_loss:.5f})\")\n                                                                                                                                                                                                               \n    return model, train_loss\n\nclass NFLRushNet(nn.Module):                                   \n    def __init__(self, dropout=0.3):                           \n        super().__init__()                                     \n        target_size = YARDS_CLIP[1] - YARDS_CLIP[0] + 1        \n        n_channels = 10                                        \n        h, w = 11, 10 # (offense, defense)                     \n        lambda_ = 0.7                                          \n                                                               \n        self.net1 = nn.ModuleList([                            \n            nn.Sequential(                                     \n                nn.Conv2d(n_channels, 128, kernel_size=1),     \n                nn.ReLU()                                      \n            ),                                                 \n            nn.Sequential(                                     \n                nn.Conv2d(128, 160, kernel_size=1),            \n                nn.ReLU()                                      \n            ),                                                 \n            nn.Sequential(                                     \n                nn.Conv2d(160, 128, kernel_size=1),            \n                nn.ReLU()                                      \n            )                                                  \n        ])                                                     \n                                                               \n        # pool over offense                                    \n        self.max_pool1 = nn.MaxPool2d((h, 1))                  \n        self.avg_pool1 = nn.AvgPool2d((h, 1))                  \n                                                               \n        self.bn1 = nn.BatchNorm1d(128)                         \n                                                               \n        self.net2 = nn.ModuleList([                            \n            nn.Sequential(                                     \n                nn.Conv1d(128, 160, kernel_size=1, bias=False),\n                nn.ReLU(),                                     \n                nn.BatchNorm1d(160)                            \n            ),                                                 \n            nn.Sequential(                                     \n                nn.Conv1d(160, 96, kernel_size=1, bias=False), \n                nn.ReLU(),                                     \n                nn.BatchNorm1d(96)                             \n            ),                                                 \n            nn.Sequential(                                     \n                nn.Conv1d(96, 96, kernel_size=1, bias=False),  \n                nn.ReLU(),                                     \n                nn.BatchNorm1d(96)                             \n            )                                                  \n        ])                                                     \n                                                               \n        self.max_pool2 = nn.MaxPool1d(w)                       \n        self.avg_pool2 = nn.AvgPool1d(w)                       \n                                                               \n                                                               \n        self.net3 = nn.ModuleList([                            \n            nn.Sequential(                                     \n                nn.Linear(96, 96, bias=False),                 \n                nn.ReLU(),                                     \n                nn.BatchNorm1d(96)                             \n            ),                                                 \n            nn.Sequential(                                     \n                nn.Linear(96, 256, bias=False),                \n                nn.ReLU(),                                     \n                nn.LayerNorm(256)                              \n            ),                                                 \n        ])                                                     \n                                                               \n        self.do = nn.Dropout(dropout)                          \n        self.out = nn.Linear(256, target_size)                 \n                                                               \n        self.apply(self.weights_init)\n        \n    def forward(self, inp):                                                 \n\n        x = inp                                                             \n        for i in range(len(self.net1)):                                     \n            x = self.net1[i](x)                                             \n\n        \n        x = (0.3*self.max_pool1(x) + 0.7*self.avg_pool1(x)).squeeze(2)       \n\n        x = self.bn1(x)                                                     \n\n        for i in range(len(self.net2)):                                     \n            x = self.net2[i](x)                                             \n\n        x = (0.3*self.max_pool2(x) + 0.7*self.avg_pool2(x)).squeeze(-1)       \n\n        for i in range(len(self.net3)):                                     \n            x = self.net3[i](x)                                             \n\n        x = self.do(x)                                                      \n\n        x = self.out(x)                                                     \n\n        return x                                                            \n        #return F.softmax(x, dim=-1)                                        \n\n    def wnorm(self):                                                        \n        self.conv = weight_norm(self.conv, name=\"weight\")                   \n        for i in range(len(self.pre)):                                      \n            self.pre[i] = weight_norm(self.pre[i], name=\"weight\")           \n\n    @staticmethod                                                           \n    def init_weight(weight):                                                \n        nn.init.uniform_(weight, -0.1, 0.1)                                 \n\n    @staticmethod                                                           \n    def init_bias(bias):                                                    \n        nn.init.constant_(bias, 0.0)                                        \n\n    @staticmethod                                                           \n    def weights_init(m):                                                    \n        classname = m.__class__.__name__                                    \n        if classname.find('Linear') != -1 or classname.find('Conv2d') != -1:\n            if hasattr(m, 'weight') and m.weight is not None:               \n                NFLRushNet.init_weight(m.weight)                            \n                if hasattr(m, 'bias') and m.bias is not None:               \n                    NFLRushNet.init_bias(m.bias)\n                    \nclass RushDataset(Dataset):                                                   \n   def __init__(self, X, X_aug, y, mask, aug=True, aug_p=0.5, tta=False):    \n       D = [X, X_aug, y, mask]                                               \n       assert all(d.shape[0] == D[0].shape[0] for d in D)                    \n                                                                             \n       self.X = [torch.Tensor(X_aug), torch.Tensor(X)]                       \n       self.y = torch.Tensor(y)                                              \n       self.mask = torch.Tensor(mask)                                        \n       self.aug = aug                                                        \n       self.aug_p = aug_p                                                    \n       self.tta = tta                                                        \n                                                                             \n   def __getitem__(self, idx):                                               \n       if not self.tta:                                                 \n           xix = int(self.aug and np.random.binomial(1, self.aug_p))         \n           return self.X[xix][idx], self.y[idx], self.mask[idx]              \n       else:                                                                 \n           return self.X[0][idx], self.X[1][idx], self.y[idx], self.mask[idx]\n                                                                             \n   def __len__(self):                                                        \n       return self.X[0].shape[0] \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/nfl-big-data-bowl-2020/train.csv', low_memory=False)\nD = preprocess(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, X_aug, y, mask, groups, idx_2017 = D\nmodels = run_final(X, X_aug, y, mask, batch_size=64, n_bagging=n_bagging)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle.competitions import nflrush\nimport pandas as pd\n\n# You can only call make_env() once, so don't lose it!\nenv = nflrush.make_env()\niter_test = env.iter_test()\n\ndef pred(test_df, sample_prediction_df):\n    D_test = preprocess(test_df, predicting=True)\n    X, X_aug, *_ = D_test\n    X = torch.Tensor(X)\n    X_aug = torch.Tensor(X_aug)\n    preds = []\n    for m in models:\n        m.eval()\n        with torch.no_grad():\n            preds.append(m(X) * 0.5 + m(X_aug) * 0.5)\n            #preds.append(F.softmax(m(X), dim=-1) * 0.5 + F.softmax(m(X_aug), dim=-1) * 0.5)\n    preds = torch.cat(preds, dim=0).mean(dim=0)\n    preds = F.softmax(preds, dim=-1).cumsum(dim=-1)\n    sample_prediction_df.iloc[0] = 0.\n    sample_prediction_df.iloc[0, 84:150] = preds.numpy()\n    sample_prediction_df.iloc[0, 150:] = 1.\n    sample_prediction_df.clip(0., 1., inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for test_df, sample_prediction_df in iter_test:\n    pred(test_df, sample_prediction_df)\n    env.predict(sample_prediction_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env.write_submission_file()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We've got a submission file!\nimport os\nprint([filename for filename in os.listdir('/kaggle/working') if '.csv' in filename])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}